{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d066fbc0-ffc4-4c63-9524-76fbf4b01980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f69ae6a-a849-4380-bed9-0e5628d59316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ac3ebe-6e7d-494c-a327-4ee59bd14810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from data.scannet.utils_scannet import ScanNetDataset\n",
    "from matching.gmflow_dense.gmflow_dense import GMflowDensePoseDeterministic\n",
    "from flow_regressors.regressors import DensePoseRegressorV7\n",
    "\n",
    "from training.loss_pose import LossPose\n",
    "from training.loss_deterministic import DeterministicLossMixed\n",
    "from training.train_deterministic import train, validate\n",
    "from utils.model import load_checkpoint\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad3027-9400-4947-a251-25f4c2468f8b",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318ac11c-b372-423e-bd1d-1763a3e07860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = ScanNetDataset(\n",
    "    root_dir='/home/project/data/scans/',\n",
    "    npz_path='/home/project/code/data/scannet_splits/smart_sample_train_ft.npz',\n",
    "    intrinsics_path='/home/project/ScanNet/scannet_indices/intrinsics.npz',\n",
    "    calculate_flow=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True, drop_last=True, pin_memory=True, num_workers=0)\n",
    "\n",
    "val_data = ScanNetDataset(\n",
    "    root_dir='/home/project/data/scans/',\n",
    "    npz_path='/home/project/code/data/scannet_splits/smart_sample_val.npz',\n",
    "    intrinsics_path='/home/project/ScanNet/scannet_indices/intrinsics.npz',\n",
    "    calculate_flow=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=2, shuffle=False, drop_last=False, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1978016-6fd1-4d79-9314-f9da9661e6cf",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c73c554-3f76-4707-8b44-1bde598b2eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "config = dict(\n",
    "    general = dict(\n",
    "        experiment_name='DETERMINISTIC_FT_part2',\n",
    "        device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "        \n",
    "        n_epochs=5,\n",
    "        n_steps_per_epoch=len(train_loader.dataset)//train_loader.batch_size,\n",
    "        n_accum_steps=16,\n",
    "        batch_size=train_loader.batch_size,\n",
    "        \n",
    "        swa=False,\n",
    "        n_epochs_swa=None,\n",
    "        n_steps_between_swa_updates=None,\n",
    "\n",
    "        repeat_val_epoch=1,\n",
    "        repeat_save_epoch=1,\n",
    "          \n",
    "        model_save_path='../../src/weights/DETERMINISTIC_FT_part2'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "config['general']['n_warmup_steps'] = int(config['general']['n_steps_per_epoch'] * 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a6d0c-8c63-467b-9075-ce7314fa4e6b",
   "metadata": {},
   "source": [
    "### 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99327e5c-129e-445d-9f01-7b72e2d1a246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = load_checkpoint('/home/project/code/src/weights/DETERMINISTIC_FT_4.pth',\n",
    "                             config['general']['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcb5f1c-7994-4781-9e63-cf75dd580ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressor = DensePoseRegressorV7(init_loss_weights=[0.0, -3.0, -3.0])\n",
    "model = GMflowDensePoseDeterministic(regressor, fine_tuning=False)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "model.to(torch.float32)\n",
    "model.to(config['general']['device']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde23ebe-d4a2-4ef3-a086-bf624f9cbad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    if 'backbone' not in name:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c95b79-d108-4075-8e92-934f121bc298",
   "metadata": {},
   "source": [
    "### 4. Loss, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6e7aa3-8426-450b-a611-ab05ebb86468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss = DeterministicLossMixed()\n",
    "val_loss = LossPose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0404e842-ad97-4ac7-a2e1-51af0b9a71f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_lr = 1e-4\n",
    "factor = 0.8\n",
    "weight_decay = 1e-6\n",
    "n_layers = 6\n",
    "no_decay = \"bias\"\n",
    "\n",
    "pattern = re.compile('layers.[0-9]+')\n",
    "opt_parameters = []\n",
    "for name, module in model.named_parameters():\n",
    "    layer = pattern.findall(name)\n",
    "    \n",
    "    if layer:\n",
    "        layer_n = int(layer[0].split('.')[1]) + 1\n",
    "        lr = max_lr * factor ** (n_layers - layer_n)\n",
    "    else:\n",
    "        lr = max_lr\n",
    "         \n",
    "    opt_parameters.append({\n",
    "        'params': module,\n",
    "        'weight_decay': 0.0 if (no_decay in name) else weight_decay,\n",
    "        'lr': lr\n",
    "    })\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecaedfe6-d3a2-41b5-ba94-c2ad90bc3f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = get_scheduler(\n",
    "    \"cosine\",    \n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config['general']['n_warmup_steps'],\n",
    "    num_training_steps=config['general']['n_steps_per_epoch'] * config['general']['n_epochs']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a85bb-a0df-437d-a18b-63284d2d4daf",
   "metadata": {},
   "source": [
    "### 6. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca7e847c-0faa-4d04-ace8-03ee36794503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkovanic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/project/code/notebooks/pose_from_coarse_flow/wandb/run-20230518_222557-oaq5w6wh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kovanic/Diploma/runs/oaq5w6wh' target=\"_blank\">DETERMINISTIC_FT_part2</a></strong> to <a href='https://wandb.ai/kovanic/Diploma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kovanic/Diploma' target=\"_blank\">https://wandb.ai/kovanic/Diploma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kovanic/Diploma/runs/oaq5w6wh' target=\"_blank\">https://wandb.ai/kovanic/Diploma/runs/oaq5w6wh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 15/50000 [00:09<8:33:21,  1.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DETERMINISTIC_FT_part2</strong> at: <a href='https://wandb.ai/kovanic/Diploma/runs/oaq5w6wh' target=\"_blank\">https://wandb.ai/kovanic/Diploma/runs/oaq5w6wh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230518_222557-oaq5w6wh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'beta1' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeneral\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/project/code/notebooks/pose_from_coarse_flow/../../src/training/train_deterministic.py:138\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_loss, val_loss, train_loader, val_loader, config, experiment_name, device, n_epochs, n_steps_per_epoch, n_accum_steps, swa, n_epochs_swa, n_steps_between_swa_updates, repeat_val_epoch, repeat_save_epoch, batch_size, model_save_path, **args)\u001b[0m\n\u001b[1;32m    133\u001b[0m train_batch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m n_accum_steps \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m n_accum_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m n_steps_per_epoch):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# nn.utils.clip_grad_norm_(model.parameters(), 5)\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m     train_loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch_loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py:117\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    110\u001b[0m     F\u001b[38;5;241m.\u001b[39madamw(params_with_grad,\n\u001b[1;32m    111\u001b[0m             grads,\n\u001b[1;32m    112\u001b[0m             exp_avgs,\n\u001b[1;32m    113\u001b[0m             exp_avg_sqs,\n\u001b[1;32m    114\u001b[0m             max_exp_avg_sqs,\n\u001b[1;32m    115\u001b[0m             state_steps,\n\u001b[1;32m    116\u001b[0m             amsgrad,\n\u001b[0;32m--> 117\u001b[0m             \u001b[43mbeta1\u001b[49m,\n\u001b[1;32m    118\u001b[0m             beta2,\n\u001b[1;32m    119\u001b[0m             group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    120\u001b[0m             group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    121\u001b[0m             group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'beta1' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_loss, val_loss, train_loader, val_loader, config, **config['general'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
