{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed0155d-7a06-4f0c-a7b5-ab4b5256119a",
   "metadata": {},
   "source": [
    "In this notebook I precompute the coarse flow predicted with GMFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a74b8e-29ce-4de6-87f0-34b537283b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c9d8bd3-8d3a-4820-b116-3e0b430a1e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346efb35-4ef4-4b63-9af4-d84f9c599c49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from data.scannet.utils_scannet import ScanNetDataset\n",
    "\n",
    "from matching.gmflow.gmflow.gmflow import GMFlow \n",
    "from utils.model import load_checkpoint\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c77513-ceb3-4089-9e25-f9a34110e41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a97f7-cdf8-4417-b9c9-f317a0ef1e21",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17339361-ad32-4eda-ad60-535864eaaecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = ScanNetDataset(\n",
    "    root_dir='/home/project/data/ScanNet/scans/',\n",
    "    npz_path='/home/project/ScanNet/smart_sample_train.npz',\n",
    "    intrinsics_path='/home/project/ScanNet/scannet_indices/intrinsics.npz',\n",
    "    mode='train'\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "val_data = ScanNetDataset(\n",
    "    root_dir='/home/project/data/ScanNet/scans/',\n",
    "    npz_path='/home/project/code/data/scannet_splits/smart_sample_val.npz',\n",
    "    intrinsics_path='/home/project/ScanNet/scannet_indices/intrinsics.npz',\n",
    "    mode='val'\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c76d6c-d017-4b05-a4f7-615a3295179d",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feed7745-48c4-4397-bfd8-46ee7af4b793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = load_checkpoint('/home/project/code/src/matching/gmflow/weights/pretrained/gmflow_with_refine_kitti-8d3b9786.pth',\n",
    "                             device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5274a0c-02c9-4288-ab29-c1f93a38109e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GMFlow(\n",
    "    num_scales=2,\n",
    "    upsample_factor=4,\n",
    "    feature_channels=128,\n",
    "    attention_type='swin',\n",
    "    num_transformer_layers=6,\n",
    "    ffn_dim_expansion=4,\n",
    "    num_head=1,\n",
    "    fine_tuning=True\n",
    ")\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.to(device);\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610978a6-a7e2-408d-9b50-df70b7c9f2f0",
   "metadata": {},
   "source": [
    "### 3. Train pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4572aa1-e331-44b5-a0b4-dd6446756f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 5\u001b[0m         data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage0\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m      8\u001b[0m                 data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m                 data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 prop_radius_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m                )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_to_save = '/home/project/coarse_flow/train/'\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(train_loader):\n",
    "        data['image0'] = data['image0'].to(device)\n",
    "        data['image1'] = data['image1'].to(device)\n",
    "        out = model(\n",
    "                data['image0'],\n",
    "                data['image1'],\n",
    "                attn_splits_list=[2, 8],\n",
    "                corr_radius_list=[-1, 4],\n",
    "                prop_radius_list=[-1, 1]\n",
    "               )\n",
    "        \n",
    "        for i in range(data['image0'].size(0)):\n",
    "            torch.save(out['flow_preds'][0][i].float().detach().cpu(), f'{path_to_save}{data[\"pair_name\"][i]}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc8a481-221f-45c4-908c-e952d871b60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/13334 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 13333/13334 [1:52:08<00:00,  1.98it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m out \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m      8\u001b[0m         data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m         data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         prop_radius_list\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m        )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 16\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflow_preds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_to_save\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpair_name\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "path_to_save = '/home/project/coarse_flow/val/'\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(val_loader):\n",
    "        data['image0'] = data['image0'].to(device)\n",
    "        data['image1'] = data['image1'].to(device)\n",
    "        out = model(\n",
    "                data['image0'],\n",
    "                data['image1'],\n",
    "                attn_splits_list=[2, 8],\n",
    "                corr_radius_list=[-1, 4],\n",
    "                prop_radius_list=[-1, 1]\n",
    "               )\n",
    "        \n",
    "        for i in range(data['image0'].size(0)):\n",
    "            torch.save(out['flow_preds'][0][i].float().detach().cpu(), f'{path_to_save}{data[\"pair_name\"][i]}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
