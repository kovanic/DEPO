{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c006b6-26c8-4bc9-9a33-fc463928240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensePoseRegressorV5(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(DensePoseRegressorV5, self).__init__()\n",
    "        \n",
    "        self.in_ch = in_ch\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            ResNetBlock(in_ch, in_ch, 60, 80, 3, 1),\n",
    "            ResNetBlock(in_ch, in_ch, 60, 80, 3, 2, groups=2),\n",
    "            ResNetBlock(in_ch, in_ch, 30, 40, 3, 2, groups=2),\n",
    "            ResNetBlock(in_ch, in_ch, 15, 20, 3, 2, groups=2),\n",
    "            ResNetBlock(in_ch, in_ch, 8, 10, 3, 2, groups=2),    \n",
    "            nn.Conv2d(in_ch, in_ch, (4, 5), 1, 0, groups=2),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.translation_conv = nn.Conv2d(in_ch // 2, 3, 1, 1, 0)\n",
    "        self.angle_conv = nn.Conv2d(in_ch // 2, 4, 1, 1, 0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):               \n",
    "        x = self.decoder(x)\n",
    "        t = self.translation_conv(x[:, :self.in_ch // 2, ...]).squeeze(2).squeeze(2)\n",
    "        q = self.angle_conv(x[:, self.in_ch // 2:, ...]).squeeze(2).squeeze(2) #B x 4 x 1 x 1 -> B x 4\n",
    "        q[:, 0] = torch.abs(q.clone()[:, 0])\n",
    "        q = q / norm(q, ord=2, dim=1, keepdim=True)\n",
    "        return q, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4780a0a-f647-4fa1-9e54-d7d730a102c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec10c6-a854-453c-946f-7eb549b43332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b511db-011e-462b-bc6d-4172d344c028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a894e02-960a-4aa2-a00e-67858485cd50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\"\"\"\n",
    "Copy paste from https://github.com/facebookresearch/detr/blob/main/models/transformer.py\n",
    "LatentTransformerRegressor and make_fixed_pe are added.\n",
    "\n",
    "'''\n",
    "DETR Transformer class.\n",
    "Copy-paste from torch.nn.Transformer with modifications:\n",
    "    * positional encodings are passed in MHattention\n",
    "    * extra LN at the end of encoder is removed\n",
    "    * decoder returns a stack of activations from all decoding layers\n",
    "'''\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch.linalg import norm\n",
    "import math\n",
    "\n",
    "\n",
    "class LayerNormTranspose(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(LayerNormTranspose, self).__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\":param: x, torch.tensor(B, dim, N)\"\"\"\n",
    "        return self.norm(x.transpose(2, 1)).transpose(2, 1)\n",
    "\n",
    "    \n",
    "class LatentTransformerRegressor(nn.Module):\n",
    "    def __init__(self, num_queries=100, d_model=128, d_compressed=64,\n",
    "                 num_decoder_layers=6, nhead=8, dim_feedforward=2048,\n",
    "                 dropout=0.0, activation='relu', normalize_before=False,\n",
    "                 return_intermediate_dec=False, H=60, W=80, use_pos_embed=True):\n",
    "        super(LatentTransformerRegressor, self).__init__()\n",
    "        \n",
    "        d_c = d_compressed * num_queries\n",
    "        assert d_c % 128 == 0, \"d_compressed * num_queries should be divisible by 128\"\n",
    "        \n",
    "        if use_pos_embed:\n",
    "            self.register_buffer(\"pos_embed\", make_fixed_pe(H, W, d_model // 2).unsqueeze(0))\n",
    "        else:\n",
    "            self.pos_embed = None\n",
    "            \n",
    "        self.geometry_patterns = nn.Embedding(num_queries, d_model)\n",
    "    \n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        decoder_norm = None\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
    "                                          return_intermediate=return_intermediate_dec)\n",
    "                    \n",
    "        self.pose_decoder = nn.Sequential(\n",
    "            nn.Conv1d(num_queries * d_model, num_queries * d_model, 1, groups=num_queries),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(num_queries * d_model, d_c, 1, groups=num_queries),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(d_c, d_c // 4, 1, groups=2), #one half of tokens for t, another for q\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(d_c // 4, d_c // 16, 1, groups=2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(d_c // 16, d_c // 64, 1, groups=2),\n",
    "            nn.LeakyReLU(0.1))\n",
    "\n",
    "        self.final_dim = d_c // 128\n",
    "        self.q_proj = nn.Conv1d(self.final_dim, 4, 1)\n",
    "        self.t_proj = nn.Conv1d(self.final_dim, 3, 1)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    \n",
    "    def forward(self, geometry):\n",
    "        # flatten (B x d_model x H x W) -> (HW x B x d_model)\n",
    "        B, _, H, W = geometry.shape\n",
    "        geometry = geometry.flatten(2).permute(2, 0, 1)\n",
    "        if self.pos_embed is not None:\n",
    "            pos_embed = self.pos_embed.repeat(B, 1, 1, 1).flatten(2).permute(2, 0, 1) \n",
    "        else:\n",
    "            pos_embed = None\n",
    "        query_embed = self.geometry_patterns.weight.unsqueeze(1).repeat(1, B, 1)\n",
    "        tgt = torch.zeros_like(query_embed)\n",
    "        decoded_patterns = self.decoder(tgt, geometry, memory_key_padding_mask=None,\n",
    "                          pos=pos_embed, query_pos=query_embed) # (num_queries x B x d_model)\n",
    "\n",
    "        decoded_patterns = decoded_patterns.permute(1, 0, 2).flatten(1).unsqueeze(2) # (B x d_model * num_queries)\n",
    "        decoded_patterns = self.pose_decoder(decoded_patterns)\n",
    "        \n",
    "        t = self.t_proj(decoded_patterns[:, :self.final_dim, ...]).squeeze(2)\n",
    "        q = self.q_proj(decoded_patterns[:, self.final_dim:, ...]).squeeze(2) #(B x 4 x 1) -> (B x 4)\n",
    "        q[:, 0] = torch.abs(q.clone()[:, 0])\n",
    "        q = q / norm(q, ord=2, dim=1, keepdim=True)\n",
    "        \n",
    "        return q, t\n",
    "        \n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
    "                           memory_mask=memory_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                           memory_key_padding_mask=memory_key_padding_mask,\n",
    "                           pos=pos, query_pos=query_pos)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(self.norm(output))\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.pop()\n",
    "                intermediate.append(output)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        \n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self, tgt, memory,\n",
    "                     tgt_mask: Optional[Tensor] = None,\n",
    "                     memory_mask: Optional[Tensor] = None,\n",
    "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None,\n",
    "                     query_pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward_pre(self, tgt, memory,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None,\n",
    "                    query_pos: Optional[Tensor] = None):\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
    "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
    "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
    "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
    "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
    "\n",
    "\n",
    "    \n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    if 'leaky_relu' in activation:\n",
    "        slope = float(activation.split(':')[1])\n",
    "        return partial(F.leaky_relu, negative_slope=0.1)\n",
    "    \n",
    "    raise RuntimeError(F\"activation should be relu/gelu/leaky_relu, not {activation}.\")\n",
    "\n",
    "    \n",
    "\n",
    "def make_fixed_pe(H, W, dim, scale=2*math.pi, temperature = 10_000):\n",
    "\n",
    "    h = torch.linspace(0, 1, H)[:, None, None].repeat(1, W, dim)  # [0, scale]\n",
    "    w = torch.linspace(0, 1, W)[None, :, None].repeat(H, 1, dim)\n",
    "\n",
    "    dim_t = torch.arange(0, dim, 2).repeat_interleave(2)\n",
    "    dim_t = temperature ** (dim_t / dim)\n",
    "\n",
    "    h /= dim_t\n",
    "    w /= dim_t\n",
    "\n",
    "    h = torch.stack([h[:, :, 0::2].sin(), h[:, :, 1::2].cos()], dim=3).flatten(2)\n",
    "    w = torch.stack([w[:, :, 0::2].sin(), w[:, :, 1::2].cos()], dim=3).flatten(2)\n",
    "\n",
    "    pe = torch.cat((h, w), dim=2)\n",
    "    return pe.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a108b70-2319-417b-a57d-140e2813639f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(3, 128, 60, 80).cuda()\n",
    "ltd = LatentTransformerRegressor(d_model=256, d_compressed=32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35389573-7d51-4a9c-81c5-1364bacb868f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) / 10 ** 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90ae4f62-f79b-42a1-9f77-20cd20781c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.265944"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(ltd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b25a04c-9995-4451-b590-97e6ea816438",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = ltd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba9ed09-9e85-43eb-a358-b416724458c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7244, 0.6745, 0.8539],\n",
      "         [0.0664, 0.9447, 0.7453],\n",
      "         [0.2592, 0.2738, 0.3264],\n",
      "         [0.2160, 0.9934, 0.0154]],\n",
      "\n",
      "        [[0.5136, 0.6743, 0.0747],\n",
      "         [0.3754, 0.7262, 0.8991],\n",
      "         [0.9848, 0.7725, 0.7791],\n",
      "         [0.0012, 0.2228, 0.9112]]]) tensor([[0.7244, 0.6745, 0.8539, 0.0664, 0.9447, 0.7453, 0.2592, 0.2738, 0.3264,\n",
      "         0.2160, 0.9934, 0.0154],\n",
      "        [0.5136, 0.6743, 0.0747, 0.3754, 0.7262, 0.8991, 0.9848, 0.7725, 0.7791,\n",
      "         0.0012, 0.2228, 0.9112]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 4, 3)\n",
    "print(x, x.flatten(1))\n",
    "# permute(1, 0, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
